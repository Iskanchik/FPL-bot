#!/usr/bin/env python3
# fpl_bot.py
# Enterprise-grade single-file FPL Prices Bot
# - Single authoritative PricesService (parsing livefpl on-demand)
# - Bootstrap caching (bootstrap-static)
# - Upstash Redis (REST) integration for snapshots, rate-limiting and allowed users
# - Circuit breaker for LiveFPL
# - /prices (human-readable HTML), /prices.json (JSON), /price_on YYYY-MM-DD
# - Background daily baseline snapshot task + HTTP health & metrics
# - Prometheus counters + simple internal metrics dict
# - Designed to be run on Northflank / containers

from __future__ import annotations
import os
import sys
import json
import asyncio
import logging
import re
import random
import time as _time
from typing import Any, Dict, List, Optional, Tuple
from collections import defaultdict
from datetime import datetime, timedelta, timezone, time as dt_time

import httpx
from bs4 import BeautifulSoup
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from aiohttp import web

# Telegram
from telegram import Update, __version__ as PTB_VERSION
from telegram.ext import Application, CommandHandler, ContextTypes, MessageHandler, filters

# Prometheus
try:
    from prometheus_client import Counter, Gauge
except Exception:
    Counter = Gauge = None  # graceful degrade if not installed

# -------------------------
# Startup / basic config
# -------------------------
START_TIME = datetime.utcnow()

# Environment
BOT_TOKEN = os.getenv("BOT_TOKEN", "").strip()
OWNER_ID = int(os.getenv("OWNER_ID", "0"))
ALLOWED_GROUP_ID = int(os.getenv("ALLOWED_GROUP_ID", "0"))

UPSTASH_REDIS_REST_URL = os.getenv("UPSTASH_REDIS_REST_URL", "").rstrip("/")  # e.g. https://eu1-upstash-example.upstash.io
UPSTASH_REDIS_REST_TOKEN = os.getenv("UPSTASH_REDIS_REST_TOKEN", "").strip()

HTTP_TIMEOUT = int(os.getenv("HTTP_TIMEOUT", "20"))
BOOTSTRAP_TTL_SECS = int(os.getenv("BOOTSTRAP_TTL_SECS", "30"))
PRICE_CHANGE_UTC_PLUS = int(os.getenv("PRICE_CHANGE_UTC_PLUS", "5"))
METRICS_PORT = int(os.getenv("METRICS_PORT", "8080"))
RATE_LIMIT_TOKENS = int(os.getenv("RATE_LIMIT_TOKENS", "20"))
RATE_LIMIT_WINDOW = int(os.getenv("RATE_LIMIT_WINDOW", "60"))
SNAPSHOT_TTL_DAYS = int(os.getenv("SNAPSHOT_TTL_DAYS", "30"))
LIVEFPL_URL = os.getenv("LIVEFPL_URL", "https://www.livefpl.net/prices")
FPL_BOOTSTRAP = os.getenv("FPL_BOOTSTRAP", "https://fantasy.premierleague.com/api/bootstrap-static/")

WIDTH_LIMIT = int(os.getenv("WIDTH_LIMIT", "40"))

# sanity checks (fail fast)
if not BOT_TOKEN or OWNER_ID <= 0 or ALLOWED_GROUP_ID == 0:
    print("BOT_TOKEN, OWNER_ID, ALLOWED_GROUP_ID must be set", file=sys.stderr)
    # do not sys.exit here in case user intends to run non-telegram parts; but warn and continue with HTTP endpoints disabled
if (not UPSTASH_REDIS_REST_URL) or (not UPSTASH_REDIS_REST_TOKEN):
    print("UPSTASH_REDIS_REST_URL and UPSTASH_REDIS_REST_TOKEN should be set for persistence (snapshots, rate-limiting). Falling back to in-memory storage.", file=sys.stderr)

# Logging
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=LOG_LEVEL, format="%(asctime)s | %(levelname)s | %(name)s | %(message)s", stream=sys.stdout)
logger = logging.getLogger("fpl_bot")
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("telegram").setLevel(logging.WARNING)

# Prometheus metrics (if available)
if Counter and Gauge:
    MET_PRICES_REQUESTS = Counter("fpl_prices_requests_total", "Total /prices requests")
    MET_PARSE_SUCCESS = Counter("fpl_parse_success_total", "Successful LiveFPL parses")
    MET_PARSE_FAIL = Counter("fpl_parse_fail_total", "Failed LiveFPL parses")
    MET_AUTOGEN_BASELINE = Counter("fpl_autogen_baseline_total", "Autogenerated baseline count")
    MET_LIVEFPL_FAIL = Counter("fpl_livefpl_failures_total", "LiveFPL failures")
    MET_CB_OPEN = Gauge("fpl_circuit_open", "LiveFPL circuit open (1=open,0=closed)")
else:
    MET_PRICES_REQUESTS = MET_PARSE_SUCCESS = MET_PARSE_FAIL = MET_AUTOGEN_BASELINE = MET_LIVEFPL_FAIL = MET_CB_OPEN = None

# internal quick metrics
MET = defaultdict(int)

def inc_local_metric(name: str, v: int = 1):
    MET[name] += v

# -------------------------
# Upstash minimal client (REST) - resilient wrapper
# -------------------------
class UpstashClient:
    def __init__(self, base: str, token: str, timeout: int = HTTP_TIMEOUT):
        self.base = base.rstrip("/") if base else ""
        self.token = token
        self.timeout = timeout
        if self.base:
            self.client = httpx.AsyncClient(timeout=timeout, headers={"Authorization": f"Bearer {token}"})
        else:
            self.client = None

    async def close(self):
        if self.client:
            try:
                await self.client.aclose()
            except Exception:
                pass

    async def _get(self, path: str) -> Optional[dict]:
        if not self.client:
            return None
        url = f"{self.base}/{path}"
        try:
            r = await self.client.get(url)
        except Exception as e:
            logger.debug("Upstash transport error: %s", e)
            inc_local_metric("upstash_transport_err")
            return None
        if r.status_code != 200:
            logger.warning("Upstash status %s for %s", r.status_code, path)
            return None
        try:
            return r.json()
        except Exception:
            return None

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=4), retry=retry_if_exception_type(httpx.TransportError))
    async def hgetall(self, key: str) -> Dict[str, str]:
        j = await self._get(f"hgetall/{key}")
        if not j:
            return {}
        if isinstance(j, dict) and "result" in j and isinstance(j["result"], dict):
            return {k: str(v) for k, v in j["result"].items()}
        return {k: str(v) for k, v in (j or {}).items()}

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=4), retry=retry_if_exception_type(httpx.TransportError))
    async def hset_map(self, key: str, mapping: Dict[str, str]):
        if not mapping or not self.client:
            return
        parts = ["hset", key]
        for k, v in mapping.items():
            parts.append(k); parts.append(v)
        path = "/".join(parts)
        await self._get(path)

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=4), retry=retry_if_exception_type(httpx.TransportError))
    async def get(self, key: str) -> Optional[str]:
        j = await self._get(f"get/{key}")
        if not j:
            return None
        if isinstance(j, dict) and "result" in j:
            return None if j["result"] is None else str(j["result"])
        return None

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=4), retry=retry_if_exception_type(httpx.TransportError))
    async def set(self, key: str, value: str):
        if not self.client:
            return
        await self._get(f"set/{key}/{value}")

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=4), retry=retry_if_exception_type(httpx.TransportError))
    async def incr(self, key: str) -> Optional[int]:
        if not self.client:
            return None
        j = await self._get(f"incr/{key}")
        if not j:
            return None
        if isinstance(j, dict) and "result" in j:
            try:
                return int(j["result"])
            except Exception:
                return None
        return None

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=4), retry=retry_if_exception_type(httpx.TransportError))
    async def expire(self, key: str, seconds: int):
        if not self.client:
            return
        await self._get(f"expire/{key}/{seconds}")

_upstash = UpstashClient(UPSTASH_REDIS_REST_URL, UPSTASH_REDIS_REST_TOKEN, timeout=HTTP_TIMEOUT)

# -------------------------
# HTTP client with retry helpers
# -------------------------
class HttpClient:
    def __init__(self, timeout=HTTP_TIMEOUT):
        self._client = httpx.AsyncClient(timeout=timeout, headers={"User-Agent": "FPL-Enterprise-Bot/1.0"})

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=6), retry=retry_if_exception_type((httpx.TransportError, httpx.ReadTimeout)))
    async def get_text(self, url: str) -> Optional[str]:
        r = await self._client.get(url)
        if r.status_code != 200:
            raise httpx.HTTPStatusError("status", request=r.request, response=r)
        return r.text

    async def close(self):
        try:
            await self._client.aclose()
        except Exception:
            pass

_http = HttpClient()

# -------------------------
# Bootstrap caching (single source)
# -------------------------
_BOOTSTRAP_CACHE: Dict[str, Any] = {"ts": 0.0, "data": None, "elements": [], "el_map": {}, "name_index": {}, "events": []}
_BOOTSTRAP_LOCK = asyncio.Lock()
BOOTSTRAP_TTL = BOOTSTRAP_TTL_SECS

async def fetch_bootstrap_cached(force: bool = False) -> Optional[dict]:
    async with _BOOTSTRAP_LOCK:
        loop = asyncio.get_event_loop()
        now = loop.time()
        if _BOOTSTRAP_CACHE["data"] and not force and (now - _BOOTSTRAP_CACHE["ts"] < BOOTSTRAP_TTL):
            return _BOOTSTRAP_CACHE["data"]
        try:
            text = await _http.get_text(FPL_BOOTSTRAP)
            data = json.loads(text)
        except Exception:
            inc_local_metric("bootstrap_fetch_fail")
            logger.exception("fetch_bootstrap_cached failed")
            return None
        elements = data.get("elements", []) or []
        el_map = {}
        name_index = defaultdict(list)
        for el in elements:
            try:
                eid = int(el.get("id"))
                el_map[eid] = el
                name = str(el.get("web_name", "")).lower().strip()
                if name:
                    name_index[name].append(el)
            except Exception:
                continue
        _BOOTSTRAP_CACHE.update({"ts": now, "data": data, "elements": elements, "el_map": el_map, "name_index": name_index, "events": data.get("events", [])})
        inc_local_metric("bootstrap_refresh")
        return data

def current_season_date_range() -> Optional[Tuple[datetime, datetime]]:
    try:
        events = _BOOTSTRAP_CACHE.get("events", []) or []
        times = []
        for e in events:
            dt = e.get("deadline_time")
            if dt:
                try:
                    times.append(datetime.fromisoformat(dt.replace("Z", "+00:00")))
                except Exception:
                    pass
        if not times:
            return None
        return (min(times), max(times))
    except Exception:
        return None

# -------------------------
# Helpers: team abbreviations + helpers
# -------------------------
FPL_TEAM_ABBR = {
    1:  "ARS", 2:  "AVL", 3:  "BOU", 4:  "BRE", 5:  "BHA",
    6:  "BUR", 7:  "CHE", 8:  "CRY", 9:  "EVE", 10: "FUL",
    11: "LIV", 12: "LUT", 13: "MCI", 14: "MUN", 15: "NEW",
    16: "NFO", 17: "SHU", 18: "TOT", 19: "WHU", 20: "WOL",
}

def _pos_code_to_str(code: int) -> str:
    return {1: "GKP", 2: "DEF", 3: "MID", 4: "FWD"}.get(int(code), "")

def sanitize_name(s: Optional[str]) -> str:
    if not s:
        return ""
    s2 = re.sub(r"[\x00-\x1f\x7f]+", " ", str(s))
    s2 = re.sub(r"\s+", " ", s2).strip()
    s2 = s2.strip(" -â€“â€”_,.;:")
    return s2

def sanitize_price_val(s: Optional[str]) -> Optional[float]:
    try:
        if not s:
            return None
        t = str(s).replace("Â£", "").replace(",", ".").strip()
        return float(t)
    except Exception:
        return None

# Display width helpers (for monospace presentation)
import unicodedata
def display_width(s: str) -> int:
    w = 0
    for ch in s:
        ea = unicodedata.east_asian_width(ch)
        if ea in ("F", "W"):
            w += 2
        else:
            if ord(ch) >= 0x1F000:
                w += 2
            else:
                w += 1
    return w

def clamp_display(s: str, max_width: int) -> str:
    s = (s or "").strip()
    if display_width(s) <= max_width:
        return s
    out = ""
    cur = 0
    for ch in s:
        chw = 2 if (unicodedata.east_asian_width(ch) in ("F","W") or ord(ch) >= 0x1F000) else 1
        if cur + chw > max_width - 1:
            break
        out += ch
        cur += chw
    return out.rstrip() + "â€¦"

def shorten_name_for_width(name: str, max_len: int) -> str:
    name = (name or "").strip()
    if display_width(name) <= max_len:
        return name
    parts = name.split()
    if len(parts) >= 2:
        first = parts[0]
        last = " ".join(parts[1:])
        short = f"{first[0]}. {last}"
        if display_width(short) <= max_len:
            return short
        last_only = parts[-1]
        short2 = f"{last_only} {first[0]}."
        if display_width(short2) <= max_len:
            return short2
    return clamp_display(name, max_len)

def emoji_for_direction(direction: str) -> str:
    if not direction:
        return "â–«"
    d = direction.lower()
    if d in ("rise","up","rising"):
        return "ðŸ”¼"
    if d in ("fall","down","falling"):
        return "ðŸ”½"
    return "â–«"

def compose_table_line(team: str, pos: str, name: str, price: str, note: str) -> str:
    # produce a neat spaced line; tuned to WIDTH_LIMIT
    teamp = (team or "").upper()
    posp = (pos or "").upper()
    left = f"{teamp} {posp}".strip()
    left_c = clamp_display(left, 12)
    name_c = clamp_display(name, 20)
    price_c = clamp_display(price, 8)
    note_c = clamp_display(note, 10)
    pieces = [p for p in [left_c, name_c, price_c, note_c] if p]
    line = "  ".join(pieces)
    if display_width(line) > WIDTH_LIMIT:
        line = clamp_display(line, WIDTH_LIMIT)
    return line

# -------------------------
# Hybrid tolerant LiveFPL parser (robust heuristics)
# -------------------------
def _parse_row_tolerant(cells: List[str]) -> Dict[str, str]:
    out = {"Name": "", "Pos": "", "Team": "", "Price": "", "Target": "", "Owned by": ""}
    tokens = [c.strip() for c in cells if c and c.strip() != ""]
    if not tokens:
        return out
    # percents
    perc = [t for t in tokens if "%" in t]
    if perc:
        out["Owned by"] = perc[-1]
        if len(perc) >= 2:
            out["Target"] = perc[-2]
    # price token
    price_tok = ""
    for t in tokens:
        if "Â£" in t or re.match(r"^\d+(\.\d+)?$", t):
            price_tok = t
            break
    if price_tok:
        out["Price"] = price_tok.replace("Â£", "").strip()
    # role tokens and candidates
    role_tokens = {"GKP","DEF","MID","FWD","GK","DF","MF","FW"}
    cand = []
    for t in tokens:
        if t == price_tok or "%" in t:
            continue
        if any(rt in t for rt in role_tokens):
            parts = t.split()
            for p in parts:
                pu = p.upper()
                if pu in role_tokens and not out["Pos"]:
                    out["Pos"] = pu
                elif re.match(r"^\d+(\.\d+)?$", p) and not out["Price"]:
                    out["Price"] = p
                else:
                    cand.append(p)
            continue
        cand.append(t)
    if cand:
        joined = " ".join(cand)
        joined = re.sub(r"\s*\(.*?\)", "", joined)
        joined = joined.replace("*", "").strip()
        out["Name"] = sanitize_name(joined)
    else:
        for t in tokens:
            if t == price_tok or "%" in t:
                break
            if re.search(r"[A-Za-zÐ-Ð¯Ð°-Ñ]", t):
                out["Name"] = sanitize_name(t)
                break
    # fallback team
    if not out["Team"] and len(cells) >= 2:
        c1 = cells[1].strip()
        if len(c1) <= 4 and c1.isalpha():
            out["Team"] = c1.upper()
    if not out["Team"] and len(cells) >= 3:
        c2 = cells[2].strip()
        if len(c2) <= 4 and c2.isalpha():
            out["Team"] = c2.upper()
    return out

def _find_section_nodes(soup: BeautifulSoup, hints: List[str]):
    nodes = {}
    candidates = soup.find_all(["h1","h2","h3","h4","strong","caption","p","div","section"])
    for hint in hints:
        hint_low = hint.lower()
        node = None
        for c in candidates:
            try:
                txt = c.get_text(" ", strip=True).lower()
            except Exception:
                txt = ""
            if hint_low in txt:
                node = c
                break
        nodes[hint] = node
    return nodes

async def hybrid_parse_livefpl(html_text: str, hints: Optional[List[str]] = None) -> Dict[str, List[Dict[str, Any]]]:
    if hints is None:
        hints = ["Already reached target", "Projected to reach target", "Others who will be close", "Predicted Rises", "Predicted Falls"]
    try:
        soup = BeautifulSoup(html_text, "html.parser")
    except Exception:
        if MET_PARSE_FAIL:
            try: MET_PARSE_FAIL.inc()
            except Exception: pass
        logger.exception("BeautifulSoup failed")
        return {}
    nodes = _find_section_nodes(soup, hints)
    sections: Dict[str, List[Dict[str, Any]]] = {}
    for hint in hints:
        node = nodes.get(hint)
        rows = []
        tbl = None
        if node:
            # try find a following table or list
            if node.name == "table":
                tbl = node
            else:
                tbl = node.find_next("table")
        if tbl:
            for tr in tbl.find_all("tr"):
                cells = tr.find_all(["td","th"])
                tds = [td.get_text(" ", strip=True) for td in cells]
                if not tds:
                    continue
                parsed = _parse_row_tolerant(tds)
                rows.append(parsed)
        else:
            # fallback: take siblings text blocks until next header
            if node:
                sibs = []
                for sib in node.find_next_siblings(limit=20):
                    if sib.name and sib.name.startswith("h"):
                        break
                    sibs.append(sib)
                for s in sibs:
                    txt = s.get_text(" ", strip=True)
                    if not txt:
                        continue
                    for part in re.split(r"\s{2,}|\n+", txt):
                        if part.strip():
                            parsed = _parse_row_tolerant([part.strip()])
                            rows.append(parsed)
        # dedupe
        seen = set()
        final = []
        for r in rows:
            key = (r.get("Name","").strip().lower(), (r.get("Price") or "").strip())
            if key in seen:
                continue
            seen.add(key)
            if (r.get("Name") or "").strip():
                final.append(r)
        sections[hint] = final
    # rises/falls map
    name_dir_map = {}
    for r in sections.get("Predicted Rises", []) or []:
        n = (r.get("Name") or "").strip()
        if n:
            name_dir_map[n.lower()] = "rise"
    for r in sections.get("Predicted Falls", []) or []:
        n = (r.get("Name") or "").strip()
        if n:
            name_dir_map[n.lower()] = "fall"
    # enrich with bootstrap (if possible)
    try:
        data = await fetch_bootstrap_cached()
        elements = data.get("elements", []) if data else []
        name_index = _BOOTSTRAP_CACHE.get("name_index", {})
    except Exception:
        elements = []
        name_index = {}
    for title, rows in list(sections.items()):
        enriched = []
        for r in rows:
            name = (r.get("Name") or "").strip()
            team_hint = (r.get("Team") or "").strip()
            pos_hint = (r.get("Pos") or "").strip()
            price_hint = sanitize_price_val(r.get("Price"))
            found = None
            try:
                # simple match using name_index
                candidates = name_index.get(name.lower(), []) if name_index else []
                if candidates:
                    found = candidates[0]
                else:
                    # fallback linear search
                    for el in elements:
                        if str(el.get("web_name","")).lower().strip() == name.lower():
                            found = el
                            break
            except Exception:
                found = None
            if found:
                try:
                    r["element"] = int(found.get("id"))
                except Exception:
                    r["element"] = found.get("id")
                try:
                    tc = found.get("team_code") or found.get("team")
                    if tc is not None:
                        try:
                            tc_int = int(tc)
                            r["Team"] = FPL_TEAM_ABBR.get(tc_int, r.get("Team") or "UNK")
                        except Exception:
                            if isinstance(tc, str) and tc.strip():
                                r["Team"] = tc.strip()
                except Exception:
                    pass
            nlow = (name or "").lower()
            if name_dir_map.get(nlow):
                r["_direction"] = name_dir_map[nlow]
            else:
                tperc = r.get("Target", "")
                try:
                    tv = float(str(tperc).replace("%", "").strip()) if tperc else 0.0
                    r["_direction"] = "rise" if tv > 0 else ("fall" if tv < 0 else "neutral")
                except Exception:
                    r["_direction"] = "neutral"
            enriched.append(r)
        sections[title] = enriched
    if MET_PARSE_SUCCESS:
        try: MET_PARSE_SUCCESS.inc()
        except Exception: pass
    return sections

# -------------------------
# Prices formatting (human)
# -------------------------
def format_prices_human(sections: Dict[str, List[Dict[str, Any]]]) -> str:
    """
    Compose improved table-style output with emoji headings.
    """
    parts = []
    order = [
        ("ðŸ”¼ Predicted Rises", "Predicted Rises"),
        ("ðŸ”½ Predicted Falls", "Predicted Falls"),
        ("ðŸ“ˆ Projected to reach target", "Projected to reach target"),
        ("â–« Others who will be close", "Others who will be close"),
    ]
    for header, key in order:
        rows = sections.get(key, []) or []
        parts.append(f"<b>{header}</b>")
        if not rows:
            parts.append("(none)")
            parts.append("")  # blank line
            continue
        for r in rows:
            team_abbr = "UNK"
            el_id = None
            for k in ("element","id"):
                try:
                    if k in r:
                        el_id = int(r[k])
                        break
                except Exception:
                    pass
            if el_id and _BOOTSTRAP_CACHE.get("el_map"):
                try:
                    tc = _BOOTSTRAP_CACHE["el_map"].get(el_id, {}).get("team_code")
                    team_abbr = FPL_TEAM_ABBR.get(int(tc), "UNK")
                except Exception:
                    pass
            else:
                tcol = (r.get("Team") or "").strip()
                if tcol:
                    team_abbr = tcol.upper()
            pos = (r.get("Pos") or "").upper() or ""
            name = (r.get("Name") or "")
            price_val = r.get("Price") or ""
            try:
                pv = float(str(price_val))
                price = f"Â£{pv:.1f}"
            except Exception:
                price = f"Â£{price_val}".strip()
            tgt_raw = r.get("Target") or r.get("Owned by") or ""
            try:
                tnum = float(str(tgt_raw).replace("%","").strip()) if tgt_raw else 0.0
                tgt_pct = f"{int(round(tnum))}%"
            except Exception:
                tgt_pct = (tgt_raw or "").strip()
            direction = r.get("_direction", "neutral")
            em = emoji_for_direction(direction)
            note = f"{em} {tgt_pct}".strip()
            line = compose_table_line(team_abbr, pos, name, price, note)
            parts.append(line)
        parts.append("")  # blank line between blocks
    return "<pre>" + "\n".join(parts) + "</pre>"

def prices_sections_to_json(sections: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
    # convert sections to JSON-serializable structures with selected fields
    out = {}
    for k, v in sections.items():
        out[k] = []
        for r in v:
            out[k].append({
                "name": r.get("Name"),
                "team": r.get("Team"),
                "pos": r.get("Pos"),
                "price": r.get("Price"),
                "target": r.get("Target") or r.get("Owned by"),
                "direction": r.get("_direction", "neutral"),
                "element": r.get("element"),
            })
    return out

# -------------------------
# Baseline persistence (Upstash-backed with local fallback)
# -------------------------
async def save_baseline_map(new_map: Dict[str, int], date_iso: Optional[str] = None):
    try:
        mapping = {str(k): str(v) for k, v in new_map.items()}
        if mapping:
            await _upstash.hset_map("fpl:prices:current", mapping)
        if date_iso is None:
            today_str = datetime.utcnow().astimezone(timezone(timedelta(hours=PRICE_CHANGE_UTC_PLUS))).date().isoformat()
        else:
            today_str = date_iso
        snap_key = f"fpl:prices:{today_str}"
        await _upstash.hset_map(snap_key, mapping)
        await _upstash.expire(snap_key, SNAPSHOT_TTL_DAYS * 24 * 3600)
    except Exception:
        # fallback: save to local file
        try:
            pdir = os.getenv("SNAPSHOT_DIR", "/app/fpl_snapshots")
            os.makedirs(pdir, exist_ok=True)
            p = os.path.join(pdir, f"{date_iso or 'auto'}.json")
            with open(p, "w", encoding="utf-8") as fh:
                json.dump({str(k): int(v) for k, v in new_map.items()}, fh)
        except Exception:
            logger.exception("save_baseline_map failed")

async def _load_daily_baseline_async(date_str: str) -> Optional[Dict[str, int]]:
    key = f"fpl:prices:{date_str}"
    try:
        d = await _upstash.hgetall(key)
        if not d:
            # local fallback
            pdir = os.getenv("SNAPSHOT_DIR", "/app/fpl_snapshots")
            p = os.path.join(pdir, f"{date_str}.json")
            if os.path.exists(p):
                with open(p, "r", encoding="utf-8") as fh:
                    data = json.load(fh)
                return {str(k): int(v) for k, v in data.items()}
            return None
        clean = {}
        for k, v in d.items():
            try:
                if isinstance(v, (int, float)):
                    clean[str(k)] = int(v)
                    continue
                sv = str(v).strip()
                if sv == "" or sv.startswith("[") or sv.startswith("{"):
                    continue
                clean[str(k)] = int(float(sv))
            except Exception:
                continue
        return clean
    except Exception:
        logger.exception("load daily baseline failed")
        return None

async def bootstrap_snapshot_for_date(date_iso: str) -> Optional[Dict[str, int]]:
    try:
        data = await fetch_bootstrap_cached(force=True)
        if not data:
            return None
        try:
            req_date = datetime.fromisoformat(date_iso).date()
        except Exception:
            return None
        tz_base = timezone(timedelta(hours=PRICE_CHANGE_UTC_PLUS))
        today_local = datetime.utcnow().astimezone(tz_base).date()
        if req_date > today_local:
            logger.warning("Refuse to create baseline for future date %s (today %s)", date_iso, today_local.isoformat())
            return None
        drange = current_season_date_range()
        if drange:
            start_dt, end_dt = drange
            if not (start_dt.date() <= req_date <= end_dt.date()):
                logger.warning("Refuse to create baseline for date outside season: %s", date_iso)
                return None
        elements = data.get("elements", []) or []
        new_map = {str(el["id"]): int(el.get("now_cost", 0)) for el in elements}
        await save_baseline_map(new_map, date_iso=date_iso)
        if MET_AUTOGEN_BASELINE:
            try: MET_AUTOGEN_BASELINE.inc()
            except Exception: pass
        return new_map
    except Exception:
        logger.exception("bootstrap_snapshot_for_date failed")
        return None

# -------------------------
# PriceDetectorService (single canonical service)
# -------------------------
class CircuitBreakerSimple:
    def __init__(self, fail_threshold: int = 3, cooldown_seconds: int = 600):
        self.fail_threshold = fail_threshold
        self.cooldown_seconds = cooldown_seconds
        self.fail_count = 0
        self.open_until = 0

    def record_success(self):
        self.fail_count = 0
        self.open_until = 0
        if MET_CB_OPEN:
            try: MET_CB_OPEN.set(0)
            except Exception: pass

    def record_failure(self):
        self.fail_count += 1
        if self.fail_count >= self.fail_threshold:
            self.open_until = int(_time.time()) + self.cooldown_seconds + random.randint(0, 30)
            logger.warning("Circuit breaker opened for %ds after %d failures", self.cooldown_seconds, self.fail_count)
            inc_local_metric("circuit_opened")
            if MET_CB_OPEN:
                try: MET_CB_OPEN.set(1)
                except Exception: pass

    def is_open(self) -> bool:
        if self.open_until == 0:
            return False
        if _time.time() >= self.open_until:
            self.fail_count = 0
            self.open_until = 0
            if MET_CB_OPEN:
                try: MET_CB_OPEN.set(0)
                except Exception: pass
            return False
        return True

class PricesService:
    def __init__(self):
        self.http = _http
        self.bootstrap_cb = CircuitBreakerSimple()
        self.livefpl_cb = CircuitBreakerSimple()
        self.bootstrap_cache = _BOOTSTRAP_CACHE

    async def build_prices_msg(self) -> Tuple[str, Dict[str, Any]]:
        """
        Fetch LIVEFPL page (on-demand), parse, enrich with bootstrap and return (html_msg, json_sections)
        If LiveFPL unavailable or circuit open, returns fallback/notice.
        """
        if self.livefpl_cb.is_open():
            logger.info("LiveFPL circuit open â€” returning unavailable message")
            return ("<code>LiveFPL temporarily unavailable</code>", {})
        try:
            txt = await self.http.get_text(LIVEFPL_URL)
        except Exception:
            self.livefpl_cb.record_failure()
            logger.exception("Failed to fetch LiveFPL page")
            if MET_LIVEFPL_FAIL:
                try: MET_LIVEFPL_FAIL.inc()
                except Exception: pass
            return ("<code>Could not fetch LiveFPL page.</code>", {})
        self.livefpl_cb.record_success()
        try:
            sections = await hybrid_parse_livefpl(txt)
            if not sections:
                return ("<code>No price projections parsed.</code>", {})
            # ensure bootstrap cached for enrichment display
            await fetch_bootstrap_cached()
            html = format_prices_human(sections)
            json_sections = prices_sections_to_json(sections)
            if MET_PRICES_REQUESTS:
                try: MET_PRICES_REQUESTS.inc()
                except Exception: pass
            return (html, json_sections)
        except Exception:
            logger.exception("Error building prices message")
            if MET_PARSE_FAIL:
                try: MET_PARSE_FAIL.inc()
                except Exception: pass
            return ("<code>Could not build prices message.</code>", {})

    def detect_between_maps(self, m1: Dict[str, int], m2: Dict[str, int]) -> List[Tuple[str, int, int]]:
        out = []
        try:
            for k, v2 in m2.items():
                try:
                    v1 = m1.get(k)
                    if v1 is None:
                        continue
                    if int(v1) != int(v2):
                        out.append((k, int(v1), int(v2)))
                except Exception:
                    continue
        except Exception:
            logger.exception("detect_between_maps failed")
        return out

    async def detect_changes_and_update_baseline(self) -> Optional[List[Tuple[str, int, int]]]:
        try:
            data = await fetch_bootstrap_cached(force=True)
            if not data:
                return None
            elements = data.get("elements", []) or []
            new_map = {str(el["id"]): int(el.get("now_cost", 0)) for el in elements}
        except Exception:
            logger.exception("failed to build new_map")
            return None
        try:
            pb = await _upstash.hgetall("fpl:prices:current")
            old_map = {}
            for k, v in (pb or {}).items():
                try:
                    old_map[str(k)] = int(v)
                except Exception:
                    continue
        except Exception:
            old_map = {}
        changes = self.detect_between_maps(old_map, new_map)
        if changes:
            today_str = datetime.utcnow().astimezone(timezone(timedelta(hours=PRICE_CHANGE_UTC_PLUS))).date().isoformat()
            await save_baseline_map(new_map, date_iso=today_str)
            try:
                await _upstash.hset_map("fpl:audit:price_changes", {str(int(datetime.utcnow().timestamp())): ",".join([f"{e}:{o}->{n}" for e, o, n in changes])})
            except Exception:
                pass
        else:
            if not old_map:
                today_str = datetime.utcnow().astimezone(timezone(timedelta(hours=PRICE_CHANGE_UTC_PLUS))).date().isoformat()
                await save_baseline_map(new_map, date_iso=today_str)
        return changes

price_service = PricesService()

# -------------------------
# Rate limiter (Upstash INCR + in-memory fallback)
# -------------------------
in_memory_rl: Dict[int, List[int]] = {}

async def rate_limit_check(user_id: int) -> bool:
    if _upstash.client:
        try:
            val = await _upstash.incr(f"fpl:rl:{user_id}")
            if val is None:
                raise RuntimeError("upstash incr failed")
            if val == 1:
                await _upstash.expire(f"fpl:rl:{user_id}", RATE_LIMIT_WINDOW)
            return val <= RATE_LIMIT_TOKENS
        except Exception:
            pass
    # fallback in-memory bucket
    now = int(datetime.utcnow().timestamp())
    bucket = in_memory_rl.get(user_id, [])
    bucket = [t for t in bucket if t > now - RATE_LIMIT_WINDOW]
    if len(bucket) >= RATE_LIMIT_TOKENS:
        return False
    bucket.append(now)
    in_memory_rl[user_id] = bucket
    return True

# -------------------------
# Authorization (owner + allowed group + whitelist)
# -------------------------
_allowed_users: set = set()

async def load_allowed_users():
    global _allowed_users
    try:
        au = await _upstash.hgetall("fpl:users:allowed")
        if au:
            _allowed_users = set(int(k) for k in au.keys() if str(k).isdigit())
        else:
            _allowed_users = set()
    except Exception:
        _allowed_users = set()

async def save_allowed_users():
    try:
        mapping = {str(u): "1" for u in sorted(list(_allowed_users))}
        if mapping:
            await _upstash.hset_map("fpl:users:allowed", mapping)
    except Exception:
        logger.exception("save_allowed_users failed")

async def is_authorized_update(update: Update) -> bool:
    try:
        chat = update.effective_chat
        user = update.effective_user
        if chat is None or user is None:
            return False
        uid = int(user.id)
        if uid == OWNER_ID and chat.type == "private":
            return True
        if chat.id == ALLOWED_GROUP_ID:
            if uid not in _allowed_users:
                _allowed_users.add(uid)
                asyncio.create_task(save_allowed_users())
            return True
        if chat.type == "private":
            return uid in _allowed_users or uid == OWNER_ID
        return False
    except Exception:
        logger.exception("Authorization check failed")
        return False

# -------------------------
# Handler guard decorator
# -------------------------
def handler_guard(fn):
    async def wrapper(update: Update, context: ContextTypes.DEFAULT_TYPE):
        try:
            user = update.effective_user
            uid = int(user.id) if user else None
            if uid and not await rate_limit_check(uid):
                try:
                    await update.message.reply_text("Rate limit exceeded. Try later.")
                except Exception:
                    pass
                return
            await fn(update, context)
        except Exception as e:
            logger.exception("Handler error: %s", e)
            try:
                await _upstash.hset_map("fpl:audit:errors", {str(int(datetime.utcnow().timestamp())): str(e)})
            except Exception:
                pass
            try:
                if OWNER_ID:
                    await context.application.bot.send_message(chat_id=OWNER_ID, text=f"Handler error: {e}")
            except Exception:
                pass
    return wrapper

# -------------------------
# Telegram command handlers
# -------------------------
@handler_guard
async def help_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await is_authorized_update(update):
        return
    text = (
        "FPL Prices Bot (Enterprise)\n"
        "/prices â€” show LiveFPL price projections (compact, mobile)\n"
        "/prices_json â€” show JSON price projections (alias to /prices.json)\n"
        "/price_on YYYY-MM-DD â€” show changes between that day and next day (baseline from bootstrap)\n"
        "/help â€” show this help"
    )
    try:
        if update.effective_chat.type == "private":
            await update.message.reply_text(text)
        else:
            await update.message.reply_text(text)
    except Exception:
        pass

@handler_guard
async def prices_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await is_authorized_update(update):
        return
    html, _json = await price_service.build_prices_msg()
    try:
        if update.effective_chat.type == "private":
            await update.message.reply_text(html, parse_mode="HTML", disable_web_page_preview=True)
        else:
            await context.application.bot.send_message(chat_id=ALLOWED_GROUP_ID, text=html, parse_mode="HTML", disable_web_page_preview=True)
    except Exception:
        logger.exception("Failed to send /prices message")

@handler_guard
async def prices_json_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await is_authorized_update(update):
        return
    html, json_sections = await price_service.build_prices_msg()
    try:
        await update.message.reply_text("JSON payload (first 2000 chars):\n" + json.dumps(json_sections)[:2000])
    except Exception:
        logger.exception("Failed to send /prices_json")

@handler_guard
async def price_on_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not await is_authorized_update(update):
        return
    args = context.args or []
    if not args:
        await update.message.reply_text("Usage: /price_on YYYY-MM-DD")
        return
    date_str = args[0].strip()
    try:
        req_date = datetime.fromisoformat(date_str).date()
    except Exception:
        await update.message.reply_text("Invalid date format. Use YYYY-MM-DD.")
        return
    await fetch_bootstrap_cached()
    drange = current_season_date_range()
    if drange:
        start_dt, end_dt = drange
        if not (start_dt.date() <= req_date <= end_dt.date()):
            await update.message.reply_text(f"Date {date_str} outside current season range.")
            return
    tz_base = timezone(timedelta(hours=PRICE_CHANGE_UTC_PLUS))
    today_local = datetime.utcnow().astimezone(tz_base).date()
    if req_date > today_local:
        await update.message.reply_text(f"Refuse to generate baseline for future date {date_str}.")
        return
    # load or autogen day1
    day1 = await _load_daily_baseline_async(date_str)
    if not day1:
        day1 = await bootstrap_snapshot_for_date(date_str)
        if not day1:
            await update.message.reply_text(f"No baseline for {date_str} and auto-generation failed.")
            return
    # prepare next day
    next_day_dt = req_date + timedelta(days=1)
    next_day = next_day_dt.isoformat()
    if next_day_dt > today_local:
        await update.message.reply_text(f"Cannot compare to future date {next_day}.")
        return
    day2 = await _load_daily_baseline_async(next_day)
    if not day2:
        day2 = await bootstrap_snapshot_for_date(next_day)
        if not day2:
            await update.message.reply_text(f"No baseline for {next_day} and auto-generation failed.")
            return
    changes = price_service.detect_between_maps(day1, day2)
    if not changes:
        await update.message.reply_text(f"<code>No changes between {date_str} and {next_day}</code>", parse_mode="HTML")
        return
    await fetch_bootstrap_cached()
    el_map = _BOOTSTRAP_CACHE.get("el_map", {})
    lines = [f"<pre>Price changes {date_str} â†’ {next_day}"]
    for eid, o, n in changes:
        el = el_map.get(int(eid), {})
        nm = el.get("web_name", f"id:{eid}")
        tail = f" {o/10:.1f} â†’ {n/10:.1f}"
        tail_w = display_width(tail)
        name_max = max(6, WIDTH_LIMIT - tail_w - 1)
        nm_short = shorten_name_for_width(nm, name_max)
        space_count = max(1, WIDTH_LIMIT - display_width(nm_short) - tail_w)
        lines.append(f"{nm_short}{' ' * space_count}{tail}")
    lines.append("</pre>")
    await update.message.reply_text("\n".join(lines), parse_mode="HTML")

# -------------------------
# Background: daily snapshot task + supervisor
# -------------------------
_bg_task: Optional[asyncio.Task] = None
_shutdown = False
APP_INSTANCE = None
_supervisor_task: Optional[asyncio.Task] = None

def next_daily_utc5(hour:int, minute:int, now: Optional[datetime]=None) -> datetime:
    if now is None:
        now = datetime.now(timezone.utc)
    tz_base = timezone(timedelta(hours=PRICE_CHANGE_UTC_PLUS))
    local_now = now.astimezone(tz_base)
    target_local = datetime.combine(local_now.date(), dt_time(hour, minute), tzinfo=tz_base)
    if local_now >= target_local:
        target_local = target_local + timedelta(days=1)
    return target_local.astimezone(timezone.utc)

async def sleep_until(target_dt_utc: datetime):
    while True:
        now = datetime.now(timezone.utc)
        secs = (target_dt_utc - now).total_seconds()
        if secs <= 0 or _shutdown:
            return
        await asyncio.sleep(min(secs, 60))

async def daily_snapshot_task(app: Application):
    logger.info("Daily snapshot task started")
    try:
        try:
            changes = await price_service.detect_changes_and_update_baseline()
            if changes:
                logger.info("Initial baseline updated with %d changes", len(changes))
        except Exception:
            logger.exception("Initial baseline update failed")
        while not _shutdown:
            next_run = next_daily_utc5(6, 0)
            await sleep_until(next_run)
            if _shutdown:
                break
            try:
                await price_service.detect_changes_and_update_baseline()
            except Exception:
                logger.exception("daily snapshot detect failed")
    except asyncio.CancelledError:
        logger.info("daily_snapshot_task cancelled")
    except Exception:
        logger.exception("daily_snapshot_task crashed")

def start_supervisor(loop, tasks_getter, restart_fn, interval=30):
    async def _loop():
        while not _shutdown:
            try:
                tasks = tasks_getter()
                for name, t in tasks.items():
                    if t is None or t.done():
                        logger.warning("Supervisor: task %s not running -> restarting", name)
                        try:
                            await restart_fn(name)
                            inc_local_metric("supervisor_restarts")
                        except Exception:
                            logger.exception("Supervisor failed to restart %s", name)
                await asyncio.sleep(interval)
            except asyncio.CancelledError:
                break
            except Exception:
                logger.exception("Supervisor crashed")
                await asyncio.sleep(interval)
    return asyncio.get_event_loop().create_task(_loop())

async def restart_background_task(name: str):
    global _bg_task
    if name == "daily_snapshot":
        if _bg_task and not _bg_task.done():
            _bg_task.cancel()
        _bg_task = asyncio.create_task(daily_snapshot_task(APP_INSTANCE))

# -------------------------
# HTTP server for health & JSON prices (aiohttp)
# -------------------------
async def handle_health(request):
    data = {
        "uptime_seconds": int((datetime.utcnow() - START_TIME).total_seconds()),
        "bootstrap_cached": bool(_BOOTSTRAP_CACHE.get("data")),
        "baseline_count": len(await _upstash.hgetall("fpl:prices:current") or {}) if _upstash.client else "in-memory",
        "metrics": dict(MET),
        "livefpl_circuit_open": price_service.livefpl_cb.is_open() if price_service else False,
    }
    return web.json_response(data)

async def handle_prices_json(request):
    # fetch live once (on-demand) â€” return JSON structure
    html, json_sections = await price_service.build_prices_msg()
    return web.json_response({"ok": bool(json_sections), "data": json_sections})

async def handle_root(request):
    return web.Response(text="FPL Prices Bot - service endpoints: /health, /prices.json")

async def start_http_server():
    app = web.Application()
    app.router.add_get("/", handle_root)
    app.router.add_get("/health", handle_health)
    app.router.add_get("/prices.json", handle_prices_json)
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, "0.0.0.0", METRICS_PORT)
    await site.start()
    logger.info("HTTP server started on port %s", METRICS_PORT)
    while not _shutdown:
        await asyncio.sleep(1)

# -------------------------
# Lifecycle: start/stop background tasks
# -------------------------
async def start_background_tasks(app: Application):
    try:
        await load_allowed_users()
        await fetch_bootstrap_cached()
        global _bg_task, _supervisor_task, APP_INSTANCE
        APP_INSTANCE = app
        loop = asyncio.get_event_loop()
        if _bg_task is None or _bg_task.done():
            _bg_task = asyncio.create_task(daily_snapshot_task(app))
        def tasks_getter():
            return {"daily_snapshot": _bg_task}
        _supervisor_task = start_supervisor(loop, tasks_getter, restart_background_task, interval=30)
        loop.create_task(start_http_server())
        logger.info("Background tasks started")
    except Exception:
        logger.exception("Failed to start background tasks")

async def stop_background_tasks():
    global _bg_task, _supervisor_task, _shutdown
    _shutdown = True
    tasks = [_bg_task, _supervisor_task]
    for t in tasks:
        if t:
            t.cancel()
    await asyncio.sleep(0.2)
    try:
        await _http.close()
        await _upstash.close()
    except Exception:
        pass

# -------------------------
# No-op MessageHandler
# -------------------------
async def _noop(update: Update, context: ContextTypes.DEFAULT_TYPE):
    return None

# -------------------------
# App builder and run
# -------------------------
def build_app():
    app = Application.builder().token(BOT_TOKEN).build()
    app.add_handler(CommandHandler("help", help_cmd))
    app.add_handler(CommandHandler("prices", prices_cmd))
    app.add_handler(CommandHandler("prices_json", prices_json_cmd))
    app.add_handler(CommandHandler("price_on", price_on_cmd))
    app.add_handler(MessageHandler(filters.ALL & (~filters.COMMAND), _noop))
    async def _on_start(_app: Application):
        logger.info("Bot started (PTB %s)", PTB_VERSION)
        try:
            await _app.bot.set_my_commands([
                ("help","Show help and commands"),
                ("prices","Show LiveFPL price projections"),
                ("price_on","Show changes between days"),
            ])
        except Exception:
            logger.exception("Failed to set commands")
        await start_background_tasks(_app)
    async def _on_stop(_app: Application):
        logger.info("Bot stopping")
        await stop_background_tasks()
    app.post_init = _on_start
    app.post_shutdown = _on_stop
    return app

def main():
    app = build_app()
    logger.info("Starting bot...")
    try:
        app.run_polling()
    except KeyboardInterrupt:
        pass

if __name__ == "__main__":
    main()
